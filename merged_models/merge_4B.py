import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoImageProcessor  
from transformers import AutoModelForCausalLM
from transformers import AutoModelForVision2Seq,AutoProcessor
import shutil
import os
import shutil
import torch
from transformers import AutoProcessor, AutoTokenizer

def _blend_module_params_(target_module, source_module, alpha_vl=0.9,alpha_base=0.1):
    """
    - è‡ªåŠ¨å¤„ç† weight å’Œ biasï¼ˆè‹¥å­˜åœ¨ï¼‰
    - ç¡®ä¿ dtype/device å¯¹é½
    """

    # ç”¨ state_dict é€ä¸ªå‚æ•°èåˆï¼Œæ›´ç¨³å¥ä¹Ÿèƒ½è¦†ç›–æ‰€æœ‰æƒé‡/åç½®
    tgt_sd = target_module.state_dict()
    src_sd = source_module.state_dict()

    # å½¢çŠ¶æ£€æŸ¥
    for k in tgt_sd.keys():
        if k not in src_sd:
            raise ValueError(f"æºæ¨¡å—ç¼ºå°‘å‚æ•° {k}")
        if tgt_sd[k].shape != src_sd[k].shape:
            raise ValueError(f"å‚æ•°å½¢çŠ¶ä¸åŒ¹é…: {k}, tgt={tgt_sd[k].shape}, src={src_sd[k].shape}")

    with torch.no_grad():
        for k in tgt_sd.keys():
            t = tgt_sd[k]
            s = src_sd[k].to(dtype=t.dtype, device=t.device)
            tgt_sd[k].copy_(alpha_vl * t + alpha_base * s)

    # å›å†™åˆ°æ¨¡å—
    target_module.load_state_dict(tgt_sd, strict=True)


def replace_self_attn_from_base_model(
    vl_model,
    base_model,
    start_layer=24,
    end_layer=35,
    save_dir="./merged_qwen_vl3b",
    orig_vl_model_path="/root/autodl-tmp/models/Qwen3-VL-4B-Instruct",
    alpha_vl=0.9,alpha_base=0.1
):
    """
    å°† VL æ¨¡å‹æŒ‡å®šå±‚çš„ self_attn æ¨¡å—ä¸ Base æ¨¡å‹å¯¹åº”å±‚åšæƒé‡èåˆï¼š
        new_attn = alpha_vl * vl_attn + (1 - alpha_vl) * base_attn
    å¹¶å°†å®Œæ•´æ¨¡å‹ï¼ˆå« tokenizerã€chat_templateï¼‰ä¿å­˜åˆ° save_dirã€‚

    Args:
        vl_model: Qwen2.5-VL-3B-Instruct æ¨¡å‹
        base_model: Qwen2.5-3B æ¨¡å‹
        start_layer: èµ·å§‹å±‚ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
        end_layer: ç»“æŸå±‚ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
        save_dir: ä¿å­˜è·¯å¾„
        orig_vl_model_path: åŸå§‹VLæ¨¡å‹è·¯å¾„ï¼Œç”¨äºå¤åˆ¶processor/tokenizeræ–‡ä»¶
        alpha_vl: èåˆæ—¶ VL æƒé‡ç³»æ•°ï¼ˆé»˜è®¤ 0.9ï¼‰ï¼ŒBase æƒé‡ç³»æ•°ä¸º 1 - alpha_vl
    """
    vl_layers = vl_model.model.language_model.layers
    base_layers = base_model.model.layers

    assert len(vl_layers) == len(base_layers), \
        f"âŒ å±‚æ•°ä¸åŒ¹é…ï¼šVLæœ‰{len(vl_layers)}å±‚ï¼ŒBaseæœ‰{len(base_layers)}å±‚"


    print(f"ğŸ”§ å¼€å§‹èåˆå±‚ {start_layer}~{end_layer} çš„ self_attn æ¨¡å—...")
    print(f"ğŸ“Š æ€»å±‚æ•°: {len(vl_layers)}")
    blended_layers = []

    with torch.no_grad():
        for i in range(start_layer, end_layer + 1):
            vl_attn = vl_layers[i].self_attn
            base_attn = base_layers[i].self_attn

            # å¯é€‰ï¼šæœ€å…³é”®çš„ q/k/v/o æŠ•å½±æ£€æŸ¥ï¼ˆæå‰å‘ç°ç»“æ„å·®å¼‚ï¼‰
            for name in ["q_proj", "k_proj", "v_proj", "o_proj"]:
                assert hasattr(vl_attn, name) and hasattr(base_attn, name), f"{name} ä¸å­˜åœ¨äº self_attn ä¸­"
                v_mod = getattr(vl_attn, name)
                b_mod = getattr(base_attn, name)
                assert v_mod.weight.shape == b_mod.weight.shape, f"{name}.weight å½¢çŠ¶ä¸åŒ¹é…"
                if hasattr(v_mod, "bias") and v_mod.bias is not None:
                    assert (b_mod.bias is not None) and (v_mod.bias.shape == b_mod.bias.shape), f"{name}.bias å½¢çŠ¶ä¸åŒ¹é…"

            # èåˆæ•´ä¸ª self_attn çš„ state_dictï¼ˆè¦†ç›–æ‰€æœ‰å­å‚æ•°ï¼ŒåŒ…å« biasï¼‰
            _blend_module_params_(vl_attn, base_attn, alpha_vl=alpha_vl,alpha_base=alpha_base)
            blended_layers.append(i)

    print(f"ğŸ¯ æˆåŠŸèåˆ {len(blended_layers)} å±‚ï¼š{blended_layers}")

    # === ä¿å­˜æ¨¡å‹ ===
    os.makedirs(save_dir, exist_ok=True)
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹æƒé‡åˆ°ï¼š{save_dir}")
    vl_model.save_pretrained(save_dir)
    print("âœ… æ¨¡å‹æƒé‡ä¿å­˜å®Œæˆï¼")

    # === åŒæ­¥ä¿å­˜ tokenizer / processor / chat_template ===
    print("ğŸ“¦ æ­£åœ¨å¤åˆ¶ tokenizer / processor / chat_template.json ...")
    processor = AutoProcessor.from_pretrained(orig_vl_model_path)
    tokenizer = AutoTokenizer.from_pretrained(orig_vl_model_path)

    processor.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)

    # æ‰‹åŠ¨å¤åˆ¶ chat_template.jsonï¼ˆæœ‰æ—¶ä¸ä¼šè¢« processor.save_pretrained è‡ªåŠ¨åŒ…å«ï¼‰
    src_template = os.path.join(orig_vl_model_path, "chat_template.json")
    dst_template = os.path.join(save_dir, "chat_template.json")
    if os.path.exists(src_template):
        shutil.copy(src_template, dst_template)
        print(f"âœ… å·²å¤åˆ¶ chat_template.json åˆ° {dst_template}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ° chat_template.jsonï¼Œè¯·ç¡®è®¤åŸå§‹æ¨¡å‹ç›®å½•ä¸­å­˜åœ¨ã€‚")

    print(f"ğŸ‰ æ¨¡å‹èåˆä¸ä¿å­˜å…¨éƒ¨å®Œæˆï¼š{save_dir}")
    return vl_model
import torch
from transformers import AutoModelForVision2Seq, AutoModelForCausalLM

qwen_vl_path = "/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/models/Qwen3-VL-4B-Instruct"
qwen_base_path = "/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/models/Qwen3-4B"

# === Base æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ ===
print("ğŸš€ åŠ è½½ Base æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    qwen_base_path,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="cpu"
)

# === éå† start_layer ===
for start_layer in range(19, 29):

    print(f"\nğŸ” é‡æ–°åŠ è½½ VL æ¨¡å‹ï¼ˆstart_layer={start_layer}ï¼‰")
    vl_model = AutoModelForVision2Seq.from_pretrained(
        qwen_vl_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="cpu"
    )

    save_path = f"/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/merged_models/merge_4/28/merge_{start_layer}--35+0.2base+0.8vl"

    replace_self_attn_from_base_model(
        vl_model,
        base_model,
        start_layer=start_layer,
        end_layer=35,
        save_dir=save_path,
        orig_vl_model_path=qwen_vl_path,
        alpha_vl=0.8,
        alpha_base=0.2
    )

    # === å…³é”®ï¼šé‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢ç´¯è®¡å ç”¨ ===
    del vl_model
    torch.cuda.empty_cache()

